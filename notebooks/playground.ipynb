{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fec4dd9-d6d9-4786-8bd8-30d0848ce98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/jobs/bronze/bronze_services.py\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "def spark(app):\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(app)\n",
    "        # Delta Lake integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # HDFS endpoint (from docker-compose: hdfs-namenode on port 9000)\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://hdfs-namenode:9000\")\n",
    "        # small local-friendly shuffle\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "def main(ingest_date: str, out_path: str):\n",
    "    s = spark(\"bronze-delta-sanity\")\n",
    "\n",
    "    # --- 1) tiny in-memory DataFrame -----------------------------------------\n",
    "    df = s.createDataFrame(\n",
    "        [\n",
    "            (\"a001\", \"Alice\", 29),\n",
    "            (\"b002\", \"Bob\",   41),\n",
    "            (\"c003\", \"Cara\",  35),\n",
    "        ],\n",
    "        [\"customerID\", \"name\", \"age\"]\n",
    "    ).withColumn(\"ingest_date\", F.lit(ingest_date))\n",
    "\n",
    "    # --- 2) write as Delta to HDFS -------------------------------------------\n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .partitionBy(\"ingest_date\")\n",
    "        .save(out_path))\n",
    "\n",
    "    print(f\"âœ… Wrote Delta table â†’ {out_path} (ingest_date={ingest_date})\")\n",
    "\n",
    "    # --- 3) read back & show --------------------------------------------------\n",
    "    back = s.read.format(\"delta\").load(out_path)\n",
    "    print(\"ðŸ”Ž Read-back sample:\")\n",
    "    back.where(F.col(\"ingest_date\") == ingest_date).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79ce394-520d-4eb9-8b8d-4ea7e3f77357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5d8a17-5d79-4d8f-aa93-9ccaf6d03c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote Delta table â†’ hdfs://hdfs-namenode:9000/data/delta/bronze/telco/services (ingest_date=2025-10-11)\n",
      "ðŸ”Ž Read-back sample:\n",
      "+----------+-----+---+-----------+\n",
      "|customerID|name |age|ingest_date|\n",
      "+----------+-----+---+-----------+\n",
      "|a001      |Alice|29 |2025-10-11 |\n",
      "|c003      |Cara |35 |2025-10-11 |\n",
      "|b002      |Bob  |41 |2025-10-11 |\n",
      "+----------+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(\"2025-10-11\", \"hdfs://hdfs-namenode:9000/data/delta/bronze/telco/services\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45117a2-4c4b-4821-9f14-71fde1880afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2968c0-16e6-42de-8df1-78c559669ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_USER_NAME = spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"HADOOP_USER_NAME =\", os.environ.get(\"HADOOP_USER_NAME\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1573e087-b861-4cc5-bb1d-69fb94614c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ðŸ“‚ Processing: Telco_customer_churn_demographics.csv   â†’   prefix='churn_demographics'\n",
      "====================================================================================================\n",
      "['churn_demographics_customer_id_count_gender_age_under_30_senior_citizen_married_dependents_number_of_dependents']\n",
      "âœ… Saved: hdfs://hdfs-namenode:9000//data/delta/bronze/churn_demographics\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"rename-and-save-1\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # HDFS endpoint (from docker-compose: hdfs-namenode on port 9000)\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://hdfs-namenode:9000\")\n",
    "        # small local-friendly shuffle\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "RAW_DIR = \"/data/raw/telco/telco_local/\"\n",
    "OUT_DIR = \"hdfs://hdfs-namenode:9000//data/delta/bronze/\"\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r'[^a-z0-9]+', '_', s)      # non-alnum -> _\n",
    "    s = re.sub(r'_+', '_', s)               # collapse multiple _\n",
    "    s = s.strip('_')\n",
    "    return s\n",
    "\n",
    "def make_prefix_from_filename(filename: str) -> str:\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    # remove literal 'Telco_customer' (case-insensitive), then normalize\n",
    "    base = re.sub(r'(?i)telco_customer', '', base)\n",
    "    base = normalize_token(base)\n",
    "    return base or \"file\"\n",
    "\n",
    "def uniquify(names):\n",
    "    \"\"\"Ensure names are unique by appending _1, _2 ... when needed.\"\"\"\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for n in names:\n",
    "        if n not in seen:\n",
    "            seen[n] = 0\n",
    "            out.append(n)\n",
    "        else:\n",
    "            seen[n] += 1\n",
    "            out.append(f\"{n}_{seen[n]}\")\n",
    "    return out\n",
    "\n",
    "# Ensure output root exists (Spark will create subdirs)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "files = ['Telco_customer_churn_demographics.csv']\n",
    "files.sort()\n",
    "\n",
    "for f in files:\n",
    "    path = os.path.join(RAW_DIR, f)\n",
    "    prefix = make_prefix_from_filename(f)\n",
    "    print(\"=\"*100)\n",
    "    print(f\"ðŸ“‚ Processing: {f}   â†’   prefix='{prefix}'\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # Read (lazy)\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(path)\n",
    "    )\n",
    "\n",
    "    # Build rename map\n",
    "    raw_cols = df.columns\n",
    "    norm_cols = [normalize_token(c) for c in raw_cols]\n",
    "    # prepend prefix\n",
    "    target_cols = [f\"{prefix}_{c}\" if c else prefix for c in norm_cols]\n",
    "    # ensure uniqueness (in case different raw columns normalize to same token)\n",
    "    target_cols = uniquify(target_cols)\n",
    "    print(target_cols)\n",
    "    rename_pairs = list(zip(raw_cols, target_cols))\n",
    "    # Apply renames\n",
    "    for old, new in rename_pairs:\n",
    "        if old != new:\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "\n",
    "    # Write to Parquet (Spark-friendly)\n",
    "    out_path = os.path.join(OUT_DIR, prefix)\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"overwrite\")     # idempotent reruns\n",
    "        .parquet(out_path)\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253fd36-940f-4035-81e1-a9a41ca99f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
