version: "3.8"

services:
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.1.3-java8
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=local-hadoop
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
    ports:
      - "9870:9870"   # HDFS NameNode UI
      - "9000:9000"   # fs.defaultFS RPC
    volumes:
      - nn-data:/hadoop/dfs/name
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 10s
      timeout: 5s2
      retries: 30
    networks: [bigdata]

  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.3-java8
    container_name: hdfs-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
    depends_on: [hdfs-namenode]
    ports:
      - "9864:9864"   # DataNode UI
    volumes:
      - dn-data:/hadoop/dfs/data
    networks: [bigdata]

  spark-master:
    image: spark:3.5.3
    container_name: spark-master
    depends_on: [hdfs-namenode]
    command: ["/bin/bash","-lc","/opt/spark/sbin/start-master.sh && tail -f /dev/null"]
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "7077:7077"   # Spark master RPC
      - "8080:8080"   # Spark master UI
      - "4040:4040"   # Spark App UI (client mode)
    volumes:
      - ./:/opt/project   # mount your repo
    networks: [bigdata]

  spark-worker-1:
    image: spark:3.5.3
    container_name: spark-worker-1
    depends_on: [spark-master]
    command: ["/bin/bash","-lc","/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"]
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8081"   # Spark worker UI
    networks: [bigdata]

volumes:
  nn-data:
  dn-data:

networks:
  bigdata:
